{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imranmurtaza110/RL_Lunar_lander/blob/main/lunar_lander_updated_hyperparameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7COmpwX2TnH8"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium\n",
        "!pip install swig\n",
        "!apt-get install -y python-box2d\n",
        "!pip install gymnasium[box2d]\n",
        "!pip install pyvirtualdisplay\n",
        "!apt-get update\n",
        "!apt-get install -y xvfb\n",
        "!apt-get update\n",
        "!apt-get install -y xvfb python-opengl ffmpeg swig"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "from IPython.display import display as ipy_display, HTML\n",
        "from base64 import b64encode\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "from gymnasium.wrappers import RecordVideo"
      ],
      "metadata": {
        "id": "IibNt3ShQxzC"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym"
      ],
      "metadata": {
        "id": "O7TgV7UVTy6e"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class DeepQLearning(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "      self.model = nn.Sequential(\n",
        "            nn.Linear(8, 256),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(p=0.2),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 4)\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)"
      ],
      "metadata": {
        "id": "ZLGXC8HmV8kX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "_Zz66l13BLkD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LossFunction:\n",
        "  def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "\n",
        "  def step(self, loss):\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n"
      ],
      "metadata": {
        "id": "dUgmx8GFeQpD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "class Agent:\n",
        "  def __init__(self, action_space, observation_space, model):\n",
        "    self.action_space = action_space\n",
        "    self.observation_space = observation_space\n",
        "    self.epsilon = 0.3\n",
        "    self.epsilon_decay = 0.001\n",
        "    self.experience_buffer = deque(maxlen=500000)\n",
        "    self.model = model\n",
        "    self.min_epsilon = 0.01\n",
        "    self.optimizer = LossFunction(self.model)\n",
        "\n",
        "\n",
        "  def update_epsilon(self):\n",
        "        if self.epsilon not in [0.01]:\n",
        "          if self.epsilon > self.min_epsilon:\n",
        "            self.epsilon -= self.epsilon_decay\n",
        "          elif self.epsilon < 0:\n",
        "            self.epsilon = self.min_epsilon\n",
        "        # self.epsilon -= self.epsilon_decay\n",
        "        return self.epsilon\n",
        "\n",
        "\n",
        "  def select_action(self, state):\n",
        "    if np.random.random() < self.epsilon:\n",
        "        return np.random.randint(self.action_space)\n",
        "    else:\n",
        "        if not state[1]:\n",
        "          state_tensor = torch.FloatTensor(state[0][0:8]).unsqueeze(0)\n",
        "        else:\n",
        "          state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "\n",
        "        act_values = self.model(state_tensor)\n",
        "        return torch.argmax(act_values).item()\n",
        "\n",
        "\n",
        "  def experience_replay_buffer(self, state, action, reward, next_state, done):\n",
        "      self.experience_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "\n",
        "  def play(self, batch_size, gamma):\n",
        "        if len(self.experience_buffer) < batch_size:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.experience_buffer, batch_size)\n",
        "\n",
        "        states = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        next_states = []\n",
        "        is_done = []\n",
        "\n",
        "        for experience in batch:\n",
        "            state, action, reward, next_state, done = experience\n",
        "            states.append(experience[0])\n",
        "            actions.append(experience[1])\n",
        "            rewards.append(experience[2])\n",
        "            next_states.append(experience[3])\n",
        "            is_done.append(experience[4])\n",
        "\n",
        "        states_flat = []\n",
        "        for entry in states:\n",
        "            if isinstance(entry, tuple) and len(entry) == 2:  # Check if entry is a nested tuple\n",
        "                states_flat.append(entry[0])  # Append the array from the nested tuple\n",
        "            else:\n",
        "                states_flat.append(entry)  # Otherwise, append the array directly\n",
        "\n",
        "        # Convert states_flat to tensor\n",
        "        states_tensor = torch.tensor(states_flat, dtype=torch.float32)\n",
        "        actions = torch.tensor(actions, dtype=torch.long)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "        is_done = torch.tensor(is_done, dtype=torch.float32)\n",
        "\n",
        "        q_values = self.model(states_tensor)\n",
        "        next_q_values = self.model(next_states)\n",
        "\n",
        "        target_q_values = self.calculate_q_values(rewards, gamma, next_q_values, is_done)\n",
        "\n",
        "        loss = nn.MSELoss()(q_values[range(batch_size), actions], target_q_values)\n",
        "\n",
        "        self.optimizer.step(loss)\n",
        "\n",
        "  # bellman equation\n",
        "  def calculate_q_values(self, rewards, gamma, next_q_values, is_done):\n",
        "    target_q_values = []\n",
        "    for i in range(len(rewards)):\n",
        "        if is_done[i] == 0:\n",
        "            target_q = rewards[i]\n",
        "        else:\n",
        "            target_q = rewards[i] + gamma * torch.max(next_q_values[i]) * (1 - is_done[i])\n",
        "\n",
        "        target_q_values.append(target_q)\n",
        "    target_q_values = torch.tensor(target_q_values)\n",
        "    return target_q_values"
      ],
      "metadata": {
        "id": "pYDBXoK4VuZY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the environment\n",
        "import time\n",
        "\n",
        "# Register the LunarLander environment with the desired seed\n",
        "# gym.envs.register(\n",
        "#     id='LunarLander-seed-v2',\n",
        "#     entry_point='gym.envs.box2d:LunarLander',\n",
        "#     kwargs={'seed': 72}  # Pass the seed as a keyword argument\n",
        "# )\n",
        "\n",
        "\n",
        "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
        "\n",
        "\n",
        "\n",
        "# video_folder = \"/content/videos\"\n",
        "\n",
        "# folder = video_folder + f\"/episode_{2}\"\n",
        "# env = RecordVideo(env, video_folder = folder)\n",
        "# wrap the env in the record video\n",
        "# env = gym.wrappers.RecordVideo(\n",
        "#     env,\n",
        "#     \"path_to_video_folder\",  # Replace with your desired folder path\n",
        "#     name_prefix=\"video-\",\n",
        "#     # episode_trigger=lambda x: 1500 <= x <= 1600\n",
        "# )\n",
        "\n",
        "observation_shape = env.observation_space.shape\n",
        "\n",
        "num_actions = env.action_space.n\n",
        "model = DeepQLearning()\n",
        "\n",
        "# Initialize the agent\n",
        "agent = Agent(num_actions, observation_shape, model)\n",
        "\n",
        "# Main loop\n",
        "steps = []\n",
        "total_episodes = []\n",
        "step_rewards = []\n",
        "episode_rewards = []\n",
        "time_limit_seconds = 30\n",
        "# env reset for a fresh start\n",
        "observation, info = env.reset()\n",
        "# env.start_video_recorder()\n",
        "for i in range(2000):\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "    total_reward = 0\n",
        "    start_time = time.time()\n",
        "    for j in range(1000):  # Maximum episode length\n",
        "        action = agent.select_action(state)\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        agent.experience_replay_buffer(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        agent.play(batch_size=32, gamma=0.95)\n",
        "        total_reward += reward\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        # if i > 50:\n",
        "        #   if j % 50 == 0:\n",
        "        #     print('episode: ' + str(i))\n",
        "        #     print('iteration: ' + str(j))\n",
        "        #     print('action: ' + str(action))\n",
        "        #     print('reward: ' + str(reward))\n",
        "        #     print('total_reward: ' + str(total_reward))\n",
        "        #     print('\\n\\n')\n",
        "\n",
        "        if done or elapsed_time >= time_limit_seconds or j==999:\n",
        "            done = True\n",
        "            steps.append(i+1)\n",
        "            total_episodes.append(j)\n",
        "            episode_rewards.append(total_reward)\n",
        "            print('total episodes: ' + str(i))\n",
        "            print('total time: ' + str(elapsed_time))\n",
        "            print('total steps to complete the episode: ' + str(j))\n",
        "            print('reward for current state: ' + str(reward))\n",
        "            print('reward for this episode: ' + str(total_reward))\n",
        "            print('\\n')\n",
        "            break\n",
        "\n",
        "\n",
        "    # env.render()\n",
        "    epsilon_value = agent.update_epsilon()\n",
        "    print(epsilon_value)\n",
        "\n",
        "# env.close_video_recorder()\n",
        "env.close()\n"
      ],
      "metadata": {
        "id": "XW05uRUmMfta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def graph_for_episodes_with_steps():\n",
        "  x_axis = steps\n",
        "  y_axis = total_episodes\n",
        "  plt.plot(x_axis, y_axis)\n",
        "  plt.xlabel('Steps')\n",
        "  plt.ylabel('Episodes')\n",
        "  plt.title('Episodes / Steps')\n",
        "  plt.show()\n",
        "\n",
        "def graph_for_episode_rewards_with_steps():\n",
        "  x_axis = steps\n",
        "  y_axis = episode_rewards\n",
        "\n",
        "  plt.plot(x_axis, y_axis)\n",
        "  plt.xlabel('Steps')\n",
        "  plt.ylabel('Episode Reward')\n",
        "  plt.title('Episode Reward / Steps')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def graph_for_average_reward_with_steps():\n",
        "    # Calculate average reward\n",
        "    avg_rewards = [sum(episode_rewards[:i+1]) / len(episode_rewards[:i+1]) for i in range(len(episode_rewards))]\n",
        "\n",
        "    # Plotting\n",
        "    plt.plot(steps, avg_rewards)\n",
        "    plt.xlabel('Steps')\n",
        "    plt.ylabel('Average Reward')\n",
        "    plt.title('Average Reward / Steps')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "graph_for_episodes_with_steps()\n",
        "graph_for_episode_rewards_with_steps()\n",
        "graph_for_average_reward_with_steps()\n"
      ],
      "metadata": {
        "id": "Bp3WWro_vTiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/videos"
      ],
      "metadata": {
        "id": "dWu3wRwbpw9k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}