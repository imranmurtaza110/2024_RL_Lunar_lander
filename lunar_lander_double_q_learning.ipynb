{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imranmurtaza110/RL_Lunar_lander/blob/main/lunar_lander_double_q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7COmpwX2TnH8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dba53cb-2658-47f3-933a-40fdc14792c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Collecting swig\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.2.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package python-box2d\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.2.1)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376104 sha256=3d8edc1bd15ebbc8fb7918d2a5cf10db1ac10e0a3ad0566fe08c0171e1784b2d\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,077 kB]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [812 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,739 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,228 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,019 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,307 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,369 kB]\n",
            "Get:14 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:15 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:16 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [23.8 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main Sources [2,268 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main amd64 Packages [1,164 kB]\n",
            "Get:20 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [27.8 kB]\n",
            "Get:21 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [44.7 kB]\n",
            "Get:22 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [66.2 kB]\n",
            "Fetched 15.4 MB in 5s (3,302 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 209 not upgraded.\n",
            "Need to get 7,813 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.10 [28.5 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.10 [863 kB]\n",
            "Fetched 7,813 kB in 1s (6,560 kB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 131015 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.10_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.10_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package python-opengl\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install swig\n",
        "!apt-get install -y python-box2d\n",
        "!pip install gymnasium[box2d]\n",
        "!pip install pyvirtualdisplay\n",
        "!apt-get update\n",
        "!apt-get install -y xvfb\n",
        "!apt-get update\n",
        "!apt-get install -y xvfb python-opengl ffmpeg swig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IibNt3ShQxzC"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "from IPython.display import display as ipy_display, HTML\n",
        "from base64 import b64encode\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "from gymnasium.wrappers import RecordVideo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "O7TgV7UVTy6e"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZLGXC8HmV8kX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "# Deep learning model with fully connected layers\n",
        "\n",
        "\n",
        "class DeepQLearning(nn.Module):\n",
        "    def __init__(self, fc_1, fc_2, fc_3):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(8, fc_1)\n",
        "        self.fc2 = nn.Linear(fc_1, fc_2)\n",
        "        self.fc3 = nn.Linear(fc_2, fc_3)\n",
        "        self.fc4 = nn.Linear(fc_3, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        return self.fc4(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_Zz66l13BLkD"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "dUgmx8GFeQpD"
      },
      "outputs": [],
      "source": [
        "# # Loss Function class with adam optimizer\n",
        "# class LossFunction:\n",
        "#   def __init__(self, model, lr):\n",
        "#         self.model = model\n",
        "#         self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "#   def step(self, loss):\n",
        "#       self.optimizer.zero_grad()\n",
        "#       loss.backward()\n",
        "#       self.optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "pYDBXoK4VuZY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "from math import sqrt, log\n",
        "\n",
        "\"\"\"\n",
        "  Epsilon-greedy strategy is used for action selection in Agent class.\n",
        "  Manages an experience replay buffer and a neural network model for decision-making.\n",
        "  Epsilon, representing the exploration rate, decays over time but is bounded by a minimum threshold.\n",
        "  Employs the Bellman equation for updating the model based on stored experiences.\n",
        "\"\"\"\n",
        "class Agent:\n",
        "  def __init__(self, action_space, observation_space, epsilon, min_epsilon, epsilon_decay, lr, model):\n",
        "    self.action_space = action_space\n",
        "    self.observation_space = observation_space\n",
        "    self.epsilon = epsilon\n",
        "    self.epsilon_decay = epsilon_decay\n",
        "    self.experience_buffer = deque(maxlen=5000000)\n",
        "    self.model = model\n",
        "    self.target_model = DeepQLearning(256, 128, 64)\n",
        "    self.target_model.load_state_dict(model.state_dict())\n",
        "    self.min_epsilon = min_epsilon\n",
        "    self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "  def update_epsilon(self):\n",
        "        if self.epsilon not in [0.01]:\n",
        "          if self.epsilon > self.min_epsilon:\n",
        "            self.epsilon -= self.epsilon_decay\n",
        "          elif self.epsilon < 0:\n",
        "            self.epsilon = self.min_epsilon\n",
        "        return self.epsilon\n",
        "\n",
        "  def select_action(self, state):\n",
        "    if np.random.random() < self.epsilon:\n",
        "        return np.random.randint(self.action_space)\n",
        "    else:\n",
        "        if not state[1]:\n",
        "          state_tensor = torch.FloatTensor(state[0][0:8]).unsqueeze(0)\n",
        "        else:\n",
        "          state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "\n",
        "        act_values = self.model(state_tensor)\n",
        "        return torch.argmax(act_values).item()\n",
        "\n",
        "\n",
        "  def experience_replay_buffer(self, state, action, reward, next_state, done):\n",
        "      self.experience_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "\n",
        "  # def play(self, batch_size, gamma):\n",
        "  #     if len(self.experience_buffer) < batch_size:\n",
        "  #         return 0\n",
        "  #     batch = random.sample(self.experience_buffer, batch_size)\n",
        "\n",
        "  #     states = []\n",
        "  #     actions = []\n",
        "  #     rewards = []\n",
        "  #     next_states = []\n",
        "  #     is_done = []\n",
        "\n",
        "  #     for experience in batch:\n",
        "  #         state, action, reward, next_state, done = experience\n",
        "  #         states.append(experience[0])\n",
        "  #         actions.append(experience[1])\n",
        "  #         rewards.append(experience[2])\n",
        "  #         next_states.append(experience[3])\n",
        "  #         is_done.append(experience[4])\n",
        "\n",
        "  #     states_flat = []\n",
        "  #     for entry in states:\n",
        "  #         if isinstance(entry, tuple) and len(entry) == 2:\n",
        "  #             states_flat.append(entry[0])\n",
        "  #         else:\n",
        "  #             states_flat.append(entry)\n",
        "\n",
        "  #     states_tensor = torch.tensor(states_flat, dtype=torch.float32)\n",
        "  #     actions = torch.tensor(actions, dtype=torch.long)\n",
        "  #     rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "  #     next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "  #     is_done = torch.tensor(is_done, dtype=torch.float32)\n",
        "\n",
        "  #     q_values = self.model(states_tensor)\n",
        "  #     next_q_values = self.model(next_states)\n",
        "\n",
        "  #     target_q_values = self.calculate_q_values(rewards, gamma, next_q_values, is_done)\n",
        "\n",
        "  #     loss = nn.MSELoss()(q_values[range(batch_size), actions], target_q_values)\n",
        "\n",
        "  #     self.optimizer.zero_grad()\n",
        "  #     loss.backward()\n",
        "  #     self.optimizer.step()\n",
        "\n",
        "  #     return loss.item()\n",
        "\n",
        "\n",
        "  def play(self, batch_size, gamma):\n",
        "      if len(self.experience_buffer) < batch_size:\n",
        "          return 0\n",
        "      batch = random.sample(self.experience_buffer, batch_size)\n",
        "\n",
        "      states = []\n",
        "      actions = []\n",
        "      rewards = []\n",
        "      next_states = []\n",
        "      dones = []\n",
        "\n",
        "      for (state, action, reward, next_state, done) in batch:\n",
        "          states.append(state)\n",
        "          actions.append(action)\n",
        "          rewards.append(reward)\n",
        "          next_states.append(next_state)\n",
        "          dones.append(done)\n",
        "\n",
        "      states_flat = []\n",
        "      for entry in states:\n",
        "          if isinstance(entry, tuple) and len(entry) == 2:\n",
        "              states_flat.append(entry[0])\n",
        "          else:\n",
        "              states_flat.append(entry)\n",
        "\n",
        "      states_tensor = torch.tensor(states_flat, dtype=torch.float32)\n",
        "\n",
        "      states = states_tensor\n",
        "      actions = torch.tensor(actions, dtype=torch.long)\n",
        "      rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "      next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "      dones = torch.tensor(dones, dtype=torch.float32)\n",
        "\n",
        "      current_q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "      next_actions = self.model(next_states).argmax(1)\n",
        "      next_q_values = self.target_model(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
        "      expected_q_values = rewards + gamma * next_q_values * (1 - dones)\n",
        "\n",
        "      loss = nn.MSELoss()(current_q_values, expected_q_values)\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "      return loss.item()\n",
        "\n",
        "  def update_target_network(self):\n",
        "      self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "\n",
        "  # bellman equation\n",
        "  def calculate_q_values(self, rewards, gamma, next_q_values, is_done):\n",
        "    target_q_values = []\n",
        "    for i in range(len(rewards)):\n",
        "        if is_done[i] == 0:\n",
        "            target_q = rewards[i]\n",
        "        else:\n",
        "            target_q = rewards[i] + gamma * torch.max(next_q_values[i]) * (1 - is_done[i])\n",
        "\n",
        "        target_q_values.append(target_q)\n",
        "    target_q_values = torch.tensor(target_q_values)\n",
        "    return target_q_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Bp3WWro_vTiv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Different graphs are used for data visualization\n",
        "\n",
        "def plot_steps_per_episode(episodes, steps_per_episode):\n",
        "  x_axis = episodes\n",
        "  y_axis = steps_per_episode\n",
        "  plt.plot(x_axis, y_axis)\n",
        "  plt.xlabel('Episodes')\n",
        "  plt.ylabel('Steps')\n",
        "  plt.title('Steps / Episodes')\n",
        "  plt.show()\n",
        "\n",
        "def plot_rewards_per_episode(episodes, rewards):\n",
        "  x_axis = episodes\n",
        "  y_axis = rewards\n",
        "\n",
        "  plt.plot(x_axis, y_axis, '-o')\n",
        "  plt.xlabel('Episode')\n",
        "  plt.ylabel('Reward')\n",
        "  plt.title('Reward / Episode')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_cumulative_rewards(episodes, episode_rewards):\n",
        "    avg_rewards = [sum(episode_rewards[:i+1]) / len(episode_rewards[:i+1]) for i in range(len(episode_rewards))]\n",
        "    plt.plot(episodes, avg_rewards)\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Commulative Reward')\n",
        "    plt.title('Commulative Reward / Episodes')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_average_reward_per_hundred_episodes(episodes, episode_rewards):\n",
        "    num_chunks = len(episode_rewards) // 100\n",
        "\n",
        "    average_rewards = []\n",
        "    episode_indices = []\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        start_index = i * 100\n",
        "        end_index = start_index + 100\n",
        "        chunk = episode_rewards[start_index:end_index]\n",
        "        average_reward = sum(chunk) / 100\n",
        "\n",
        "        average_rewards.append(average_reward)\n",
        "        episode_indices.append(episodes[end_index - 1])\n",
        "\n",
        "    plt.plot(episode_indices, average_rewards)\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Average Reward')\n",
        "    plt.title('Average Reward Per 100 Episodes')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def plot_loss_function(episodes, loss):\n",
        "    batch_size = 50\n",
        "    num_chunks = len(loss) // batch_size\n",
        "\n",
        "    average_losses = []\n",
        "    episode_indices = []\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        start_index = i * batch_size\n",
        "        end_index = start_index + batch_size\n",
        "        chunk = loss[start_index:end_index]\n",
        "        average_loss = sum(chunk) / batch_size\n",
        "\n",
        "        average_losses.append(average_loss)\n",
        "        episode_indices.append(episodes[end_index - 1])  # End of each batch\n",
        "\n",
        "    plt.plot(episode_indices, average_losses)\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Average Loss')\n",
        "    plt.title('Average Loss Per 64 Episodes')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "XW05uRUmMfta"
      },
      "outputs": [],
      "source": [
        "# Create the environment\n",
        "import time\n",
        "\n",
        "\n",
        "def testbed(epi, steps_taken, epsilon, min_epsilon, epsilon_decay, lr, fc_1, fc_2, fc_3, batch_size, gamma):\n",
        "\n",
        "  env = gym.make(\n",
        "      \"LunarLander-v2\",\n",
        "      render_mode=\"rgb_array\",\n",
        "      continuous = False,\n",
        "\n",
        "  )\n",
        "\n",
        "  # Fixing seeds\n",
        "  gym.envs.register(\n",
        "      id='LunarLander-seed-v2',\n",
        "      entry_point='gym.envs.box2d:LunarLander',\n",
        "      kwargs={'seed': 72}  # Pass the seed as a keyword argument\n",
        "  )\n",
        "\n",
        "  video_folder = \"/content/videos\"\n",
        "\n",
        "  folder = video_folder + f\"/episode_{2}\"\n",
        "  # # wrap the env in the record video\n",
        "  env = gym.wrappers.RecordVideo(\n",
        "      env,\n",
        "      \"folder\",  # Replace with your desired folder path\n",
        "      name_prefix=\"video-\",\n",
        "      episode_trigger=lambda x: x % 50 == 0\n",
        "  )\n",
        "\n",
        "  observation_shape = env.observation_space.shape\n",
        "\n",
        "  num_actions = env.action_space.n\n",
        "  model = DeepQLearning(fc_1, fc_2, fc_3)\n",
        "\n",
        "  # Initialize the agent\n",
        "  agent = Agent(num_actions, observation_shape, epsilon, min_epsilon, epsilon_decay, lr, model)\n",
        "\n",
        "\n",
        "\n",
        "  # Main loop\n",
        "  episodes = []\n",
        "  steps_per_episode = []\n",
        "  episode_rewards = []\n",
        "  time_limit_seconds = 30\n",
        "  total_loss = []\n",
        "  # env reset for a fresh start\n",
        "  observation, info = env.reset()\n",
        "  env.start_video_recorder()\n",
        "  observation = env.reset()\n",
        "\n",
        "  for i in range(epi):\n",
        "\n",
        "    state = env.reset()\n",
        "    total_losses = 0\n",
        "    total_reward = 0\n",
        "    start_time = time.time()\n",
        "    for j in range(steps_taken):  # Maximum episode length\n",
        "        action = agent.select_action(state)\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        agent.experience_replay_buffer(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        loss = agent.play(batch_size=batch_size, gamma=gamma)\n",
        "        total_losses += loss\n",
        "        total_reward += reward\n",
        "\n",
        "        if i % 10 == 0:  # Update the target network every 10 episodes\n",
        "          agent.update_target_network()\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        if done or elapsed_time >= time_limit_seconds or j == (steps_taken-1):\n",
        "            done = True\n",
        "            episodes.append(i+1)\n",
        "            steps_per_episode.append(j)\n",
        "            episode_rewards.append(total_reward)\n",
        "            total_loss.append(total_losses)\n",
        "            print('total episodes: ' + str(i))\n",
        "            print('total time: ' + str(elapsed_time))\n",
        "            print('total steps to complete the episode: ' + str(j))\n",
        "            print('reward for current state: ' + str(reward))\n",
        "            print('reward for this episode: ' + str(total_reward))\n",
        "            print('\\n')\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #  printing graph after every 100 iterations to visualize it\n",
        "    if (i+1) % 100 == 0:\n",
        "      plot_steps_per_episode(episodes, steps_per_episode)\n",
        "      plot_rewards_per_episode(episodes, episode_rewards)\n",
        "      plot_cumulative_rewards(episodes, episode_rewards)\n",
        "      plot_average_reward_per_hundred_episodes(episodes, episode_rewards)\n",
        "      plot_loss_function(episodes, total_loss)\n",
        "\n",
        "    env.render()\n",
        "    epsilon_value = agent.update_epsilon()\n",
        "    env.close_video_recorder()\n",
        "    env.close()\n",
        "\n",
        "  return episodes, steps_per_episode, episode_rewards, total_loss\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2KjY6fJHvmym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = testbed(epi=1000, steps_taken=1000, epsilon=0.3, min_epsilon=0.01, epsilon_decay=0.001, batch_size=32, gamma=0.95, lr=0.001, fc_1=256, fc_2=128, fc_3=64)"
      ],
      "metadata": {
        "id": "yBZF6DRijPfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graphs visualization\n",
        "\n",
        "episodes, steps_per_episode, episode_rewards, total_losses = result\n",
        "print(total_losses)\n",
        "plot_steps_per_episode(episodes, steps_per_episode)\n",
        "plot_rewards_per_episode(episodes, episode_rewards)\n",
        "plot_cumulative_rewards(episodes, episode_rewards)\n",
        "plot_average_reward_per_hundred_episodes(episodes, episode_rewards)\n",
        "plot_loss_function(episodes, total_losses)"
      ],
      "metadata": {
        "id": "thkIbOq_k9CX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}