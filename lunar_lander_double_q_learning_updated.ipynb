{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imranmurtaza110/RL_Lunar_lander/blob/main/lunar_lander_double_q_learning_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7COmpwX2TnH8"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium\n",
        "!pip install swig\n",
        "!apt-get install -y python-box2d\n",
        "!pip install gymnasium[box2d]\n",
        "!pip install pyvirtualdisplay\n",
        "!apt-get update\n",
        "!apt-get install -y xvfb\n",
        "!apt-get update\n",
        "!apt-get install -y xvfb python-opengl ffmpeg swig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IibNt3ShQxzC"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "from IPython.display import display as ipy_display, HTML\n",
        "from base64 import b64encode\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "from gymnasium.wrappers import RecordVideo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "O7TgV7UVTy6e"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZLGXC8HmV8kX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Deep learning model with fully connected layers\n",
        "class DeepQLearning(nn.Module):\n",
        "  def __init__(self, fc_1, fc_2, fc_3):\n",
        "      super().__init__()\n",
        "      self.model = nn.Sequential(\n",
        "            nn.Linear(8, fc_1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc_1, fc_2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc_2, fc_3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc_3, 4)\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_Zz66l13BLkD"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dUgmx8GFeQpD"
      },
      "outputs": [],
      "source": [
        "# Loss Function class with adam optimizer\n",
        "class LossFunction:\n",
        "  def __init__(self, model, lr):\n",
        "        self.model = model\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "  def step(self, loss):\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pYDBXoK4VuZY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "from math import sqrt, log\n",
        "\n",
        "\"\"\"\n",
        "  Epsilon-greedy strategy is used for action selection in Agent class.\n",
        "  Manages an experience replay buffer and a neural network model for decision-making.\n",
        "  Epsilon, representing the exploration rate, decays over time but is bounded by a minimum threshold.\n",
        "  Employs the Bellman equation for updating the model based on stored experiences.\n",
        "\"\"\"\n",
        "class Agent:\n",
        "  def __init__(self, action_space, observation_space, epsilon, min_epsilon, epsilon_decay, lr, model, fc_1, fc_2, fc_3):\n",
        "    self.action_space = action_space\n",
        "    self.observation_space = observation_space\n",
        "    self.epsilon = epsilon\n",
        "    self.epsilon_decay = epsilon_decay\n",
        "    self.experience_buffer = deque(maxlen=5000000)\n",
        "    self.model = model\n",
        "    self.target_model = DeepQLearning(fc_1, fc_2, fc_3)\n",
        "    self.target_model.load_state_dict(model.state_dict())\n",
        "    self.min_epsilon = min_epsilon\n",
        "    self.optimizer = LossFunction(self.model, lr=lr)\n",
        "\n",
        "  def update_epsilon(self):\n",
        "        if self.epsilon not in [0.01]:\n",
        "          if self.epsilon > self.min_epsilon:\n",
        "            self.epsilon -= self.epsilon_decay\n",
        "          elif self.epsilon < 0:\n",
        "            self.epsilon = self.min_epsilon\n",
        "        return self.epsilon\n",
        "\n",
        "  def select_action(self, state):\n",
        "    if np.random.random() < self.epsilon:\n",
        "        return np.random.randint(self.action_space)\n",
        "    else:\n",
        "        if not state[1]:\n",
        "          state_tensor = torch.FloatTensor(state[0][0:8]).unsqueeze(0)\n",
        "        else:\n",
        "          state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "\n",
        "        act_values = self.model(state_tensor)\n",
        "        return torch.argmax(act_values).item()\n",
        "\n",
        "\n",
        "  def experience_replay_buffer(self, state, action, reward, next_state, done):\n",
        "      self.experience_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "\n",
        "  def play(self, batch_size, gamma):\n",
        "      if len(self.experience_buffer) < batch_size:\n",
        "          return 0\n",
        "      batch = random.sample(self.experience_buffer, batch_size)\n",
        "\n",
        "      states = []\n",
        "      actions = []\n",
        "      rewards = []\n",
        "      next_states = []\n",
        "      dones = []\n",
        "\n",
        "      for (state, action, reward, next_state, done) in batch:\n",
        "          states.append(state)\n",
        "          actions.append(action)\n",
        "          rewards.append(reward)\n",
        "          next_states.append(next_state)\n",
        "          dones.append(done)\n",
        "\n",
        "      states_flat = []\n",
        "      for entry in states:\n",
        "          if isinstance(entry, tuple) and len(entry) == 2:\n",
        "              states_flat.append(entry[0])\n",
        "          else:\n",
        "              states_flat.append(entry)\n",
        "\n",
        "      states_tensor = torch.tensor(states_flat, dtype=torch.float32)\n",
        "\n",
        "      states = states_tensor\n",
        "      actions = torch.tensor(actions, dtype=torch.long)\n",
        "      rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "      next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "      dones = torch.tensor(dones, dtype=torch.float32)\n",
        "\n",
        "      current_q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "      next_actions = self.model(next_states).argmax(1)\n",
        "      next_q_values = self.target_model(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
        "      expected_q_values = self.calculate_q_values(rewards, gamma, next_q_values, dones)\n",
        "\n",
        "\n",
        "      loss = nn.MSELoss()(current_q_values, expected_q_values)\n",
        "\n",
        "      self.optimizer.step(loss)\n",
        "\n",
        "      return loss.item()\n",
        "\n",
        "  def update_target_network(self):\n",
        "      self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "   # bellman equation\n",
        "  def calculate_q_values(self, rewards, gamma, next_q_values, is_done):\n",
        "    target_q_values = []\n",
        "    for i in range(len(rewards)):\n",
        "        if is_done[i]:\n",
        "            target_q = rewards[i]\n",
        "        else:\n",
        "            target_q = rewards[i] + gamma * torch.max(next_q_values[i]) * (1 - is_done[i])\n",
        "\n",
        "        target_q_values.append(target_q)\n",
        "    target_q_values = torch.tensor(target_q_values)\n",
        "    return target_q_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Bp3WWro_vTiv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Different graphs are used for data visualization\n",
        "\n",
        "def plot_steps_per_episode(episodes, steps_per_episode):\n",
        "  x_axis = episodes\n",
        "  y_axis = steps_per_episode\n",
        "  plt.plot(x_axis, y_axis)\n",
        "  plt.xlabel('Episodes')\n",
        "  plt.ylabel('Steps')\n",
        "  plt.title('Steps / Episodes')\n",
        "  plt.show()\n",
        "\n",
        "def plot_rewards_per_episode(episodes, rewards):\n",
        "  x_axis = episodes\n",
        "  y_axis = rewards\n",
        "\n",
        "  plt.plot(x_axis, y_axis, '-o')\n",
        "  plt.xlabel('Episode')\n",
        "  plt.ylabel('Reward')\n",
        "  plt.title('Reward / Episode')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_cumulative_rewards(episodes, episode_rewards):\n",
        "    avg_rewards = [sum(episode_rewards[:i+1]) / len(episode_rewards[:i+1]) for i in range(len(episode_rewards))]\n",
        "    plt.plot(episodes, avg_rewards)\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Commulative Reward')\n",
        "    plt.title('Commulative Reward / Episodes')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_average_reward_per_hundred_episodes(episodes, episode_rewards):\n",
        "    num_chunks = len(episode_rewards) // 100\n",
        "\n",
        "    average_rewards = []\n",
        "    episode_indices = []\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        start_index = i * 100\n",
        "        end_index = start_index + 100\n",
        "        chunk = episode_rewards[start_index:end_index]\n",
        "        average_reward = sum(chunk) / 100\n",
        "\n",
        "        average_rewards.append(average_reward)\n",
        "        episode_indices.append(episodes[end_index - 1])\n",
        "\n",
        "    plt.plot(episode_indices, average_rewards)\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Average Reward')\n",
        "    plt.title('Average Reward Per 100 Episodes')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def plot_loss_function(episodes, loss):\n",
        "    batch_size = 50\n",
        "    num_chunks = len(loss) // batch_size\n",
        "\n",
        "    average_losses = []\n",
        "    episode_indices = []\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        start_index = i * batch_size\n",
        "        end_index = start_index + batch_size\n",
        "        chunk = loss[start_index:end_index]\n",
        "        average_loss = sum(chunk) / batch_size\n",
        "\n",
        "        average_losses.append(average_loss)\n",
        "        episode_indices.append(episodes[end_index - 1])  # End of each batch\n",
        "\n",
        "    plt.plot(episode_indices, average_losses)\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Average Loss')\n",
        "    plt.title('Average Loss Per 50 Episodes')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "XW05uRUmMfta"
      },
      "outputs": [],
      "source": [
        "# Create the environment\n",
        "import time\n",
        "\n",
        "\n",
        "def testbed(epi, steps_taken, epsilon, min_epsilon, epsilon_decay, lr, fc_1, fc_2, fc_3, batch_size, gamma):\n",
        "\n",
        "  env = gym.make(\n",
        "      \"LunarLander-v2\",\n",
        "      render_mode=\"rgb_array\",\n",
        "      continuous = False,\n",
        "\n",
        "  )\n",
        "\n",
        "  # Fixing seeds\n",
        "  gym.envs.register(\n",
        "      id='LunarLander-seed-v2',\n",
        "      entry_point='gym.envs.box2d:LunarLander',\n",
        "      kwargs={'seed': 72}  # Pass the seed as a keyword argument\n",
        "  )\n",
        "\n",
        "  video_folder = \"/content/videos\"\n",
        "\n",
        "  folder = video_folder + f\"/episode_{2}\"\n",
        "  # # wrap the env in the record video\n",
        "  env = gym.wrappers.RecordVideo(\n",
        "      env,\n",
        "      \"folder\",  # Replace with your desired folder path\n",
        "      name_prefix=\"video-\",\n",
        "      episode_trigger=lambda x: x % 50 == 0\n",
        "  )\n",
        "\n",
        "  observation_shape = env.observation_space.shape\n",
        "\n",
        "  num_actions = env.action_space.n\n",
        "  model = DeepQLearning(fc_1, fc_2, fc_3)\n",
        "\n",
        "  # Initialize the agent\n",
        "  agent = Agent(num_actions, observation_shape, epsilon, min_epsilon, epsilon_decay, lr, model, fc_1, fc_2, fc_3)\n",
        "\n",
        "\n",
        "\n",
        "  # Main loop\n",
        "  episodes = []\n",
        "  steps_per_episode = []\n",
        "  episode_rewards = []\n",
        "  time_limit_seconds = 30\n",
        "  total_loss = []\n",
        "  # env reset for a fresh start\n",
        "  observation, info = env.reset()\n",
        "  env.start_video_recorder()\n",
        "  observation = env.reset()\n",
        "\n",
        "  for i in range(epi):\n",
        "\n",
        "    state = env.reset()\n",
        "    total_losses = 0\n",
        "    total_reward = 0\n",
        "    start_time = time.time()\n",
        "    for j in range(steps_taken):  # Maximum episode length\n",
        "        action = agent.select_action(state)\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        agent.experience_replay_buffer(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        loss = agent.play(batch_size=batch_size, gamma=gamma)\n",
        "        total_losses += loss\n",
        "        total_reward += reward\n",
        "\n",
        "        if i % 4 == 0:\n",
        "          agent.update_target_network()\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        if done or elapsed_time >= time_limit_seconds or j == (steps_taken-1):\n",
        "            done = True\n",
        "            episodes.append(i+1)\n",
        "            steps_per_episode.append(j)\n",
        "            episode_rewards.append(total_reward)\n",
        "            total_loss.append(total_losses)\n",
        "            print('total episodes: ' + str(i))\n",
        "            print('total time: ' + str(elapsed_time))\n",
        "            print('total steps to complete the episode: ' + str(j))\n",
        "            print('reward for current state: ' + str(reward))\n",
        "            print('reward for this episode: ' + str(total_reward))\n",
        "            print('\\n')\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #  printing graph after every 100 iterations to visualize it\n",
        "    if (i+1) % 100 == 0:\n",
        "      plot_steps_per_episode(episodes, steps_per_episode)\n",
        "      plot_rewards_per_episode(episodes, episode_rewards)\n",
        "      plot_cumulative_rewards(episodes, episode_rewards)\n",
        "      plot_average_reward_per_hundred_episodes(episodes, episode_rewards)\n",
        "      plot_loss_function(episodes, total_loss)\n",
        "\n",
        "    env.render()\n",
        "    epsilon_value = agent.update_epsilon()\n",
        "    env.close_video_recorder()\n",
        "    env.close()\n",
        "\n",
        "  return episodes, steps_per_episode, episode_rewards, total_loss\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2KjY6fJHvmym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = testbed(epi=2000, steps_taken=3000, epsilon=0.7, min_epsilon=0.01, epsilon_decay=0.001, batch_size=32, gamma=0.95, lr=0.001, fc_1=256, fc_2=128, fc_3=64)"
      ],
      "metadata": {
        "id": "yBZF6DRijPfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graphs visualization\n",
        "\n",
        "episodes, steps_per_episode, episode_rewards, total_losses = result\n",
        "plot_steps_per_episode(episodes, steps_per_episode)\n",
        "plot_rewards_per_episode(episodes, episode_rewards)\n",
        "plot_cumulative_rewards(episodes, episode_rewards)\n",
        "plot_average_reward_per_hundred_episodes(episodes, episode_rewards)\n",
        "plot_loss_function(episodes, total_losses)"
      ],
      "metadata": {
        "id": "thkIbOq_k9CX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}